# 多重线性回归

$$
Y_i=\beta_0+\sum_{i=1}^p \beta_p X_{pi}+\epsilon_i,其中\epsilon_i\sim N(0,\sigma^2)
$$

**Y**=**Xβ**+ε

在矩阵表示法中，因变量是一个向量 Y，每个样本都有一行。自变量组合成一个矩阵X，其中每个特征有一列，另外还有一列 1值用于截距。每列每个示例都有一行。回归系数β和残差ε也是向量。

向量β的最佳估计值计算为：

$$
\hat{\mathbf{\beta}}=  (X^TX)^{-1}X^TY
$$

![](images/clipboard-2956843313.png)

![](images/clipboard-2413549022.png)

```{r}
reg <- function(y, x) { 
    x <- as.matrix(x) 
    x <- cbind(Intercept = 1, x) 
    b <- solve(t(x) %*% x) %*% t(x) %*% y 
    colnames(b) <- "estimate" 
    print(b) 
}

# solve()取矩阵的逆
# t()用于转置矩阵
# %*% 将两个矩阵相乘
library(readr)
advertising<-read_csv("data/Advertising.csv")

reg(advertising$sales,advertising[c(-1,-5)])
```

## 多重线性模型

```{r}
library(tidymodels)
library(patchwork)
library(ggfortify)
library(rms)

# 检测变量相关关系
ad <- advertising %>% select(-1)
cor(ad)
```

### `rms::ols()`

```{r}

mlm_ols <- rms::ols(sales~ TV+radio+newspaper,data = advertising)
mlm_ols 
# texreg::texreg(mlm_ols)

car::vif(mlm_ols)
anova(mlm_ols)
```

### `lm()`

```{r}
mlm_lm<- lm(sales~TV+radio+newspaper,data = advertising)
summary(mlm_lm)

broom::tidy(mlm_lm)
broom::glance(mlm_lm)

logLik(mlm_lm)
car::vif(mlm_lm)





```

## 变换

### 线性组合

我们经常希望对回归系数的线性组合进行推断，特别是对于多个类别的分类变量。例如，对于分类变量，回归系数表示其中一个组与参考类别之间的平均差异

```{r}
library(multcomp, quietly = T)
confint(mlm_lm)
# 在 R 中，我们通过首先指定一个矩阵来执行此计算，该矩阵指定要进行的比较。此处的矩阵必须具有与回归方程中的回归系数相同的列数
comparison <- matrix(c(0,3,1,1), nrow=1)

lincom <- glht(mlm_lm, linfct = comparison)
summary(lincom)
confint(lincom)
```

### 交互项

```{r}
lm_interact<- lm(sales ~ .+ TV:radio, data = advertising)

lm_interact
```

### 多项式

```{r}
lm(sales ~ TV+ I(TV^2),data = advertising)
```

### 对数变换

```{r}
lm(sales ~ log(TV),data = advertising)
```

## 模型假设

<https://www.statmethods.net/stats/rdiagnostics.html>

```{r}
library(car)
car::scatterplotMatrix(ad)  # 多重共线性
confint(mlm_lm)  # 95%置信区间
plot(mlm_lm)
autoplot(mlm_lm) #回归诊断图
```

### 线性假设

残差图， 成分残差图

```{r}
plot(mlm_lm,1)  
crPlots(mlm_lm)
```

### 正态性假设Q-Q图

Standardized Residuals

```{r}
plot(mlm_lm,2) 
summary(powerTransform(mlm_lm))  
```

```{r}
plot(mlm_lm,3)
```

### 误差相关性

```{r}
durbinWatsonTest(mlm_lm)      #结果表明rho=0
```

### 误差项的方差齐性

```{r}
ncvTest(mlm_lm)
spreadLevelPlot(mlm_lm)
```

```{r}
tibble(
    abs_studentized_residuals=abs(rstudent(mlm_lm)),
    fitted_values=mlm_lm$model$sales
) %>% ggplot(aes(fitted_values,abs_studentized_residuals))+
    geom_point(pch=21)+
    geom_smooth()
```

### 异常观测点

```{r}
# studentized residual Plot
residplot<-function(fit,nbreaks=10){
  z<-rstudent(fit)
  hist(z,breaks=nbreaks,freq=FALSE)     #密度直方图
  title(xlab="Studentized Residual")
  rug(z,col="brown")                    #轴须图
  curve(dnorm(x,mean=mean(z),sd=sd(z)),add=TRUE,col="blue",lwd=2) #正态密度曲线
  lines(density(z)$x,density(z)$y,col="red",lwd=2)       #样本密度曲线
  legend("topright",c("Normal Curve","Kernel Density Curve"),#图例
  lty = c(3,2),pch = c(21,22),col=c("blue","red"),cex=.7)
}
residplot(mlm_lm)
```

```{r eval=FALSE}
#######################################################################
library(car)
outlierTest(mlm_lm)            #离群点
#高杠杆值点
hat.plot<-function(fit){
  p<-length(coefficients(fit)) #模型估计的参数数目（包含截距项）
  n<-length(fitted(fit))       #样本量
  plot(hatvalues(fit),main="Index Plot of Hat Values")#帽子值
  abline(h=c(2,3)*p/n,col="red",lty=2)  #大于帽子均值p/n的2或3倍被认为是高杠杆值
  identity(1:n,hatvalues(fit),names(hatvalues(fit)))
}
hat.plot(mlm_lm)
####强影响点
#Cook's D图形    大于4/(n-k-1)  k为预测变量数目
cutoff<-4/(nrow(advertising)-length(mlm_lm$coefficients)-2)
{plot(mlm_lm,which=4,cook.levels=cutoff)
abline(h=cutoff,lty=2,col="red")}
#变量添加图
avPlots(mlm_lm,ask=FALSE,id.method="identity")

###
influencePlot(mlm_lm,id.method="identity",main="Influence Plot")
```

### 多重共线性

```{r}
car::vif(mlm_lm)

sqrt(car::vif(mlm_lm))>=2       #vif平方根 ≥2 存在
```

## 逐步回归

逐步回归是筛选变量，有向前、向后和两个方向同时进行三个方法。

-   `direction = "both"`双向

-   `direction = "backward"`向后

-   `direction = "forward"`向前

```{r}
step_full <- lm(sales~ . ,data = advertising[-1])
step_lm_0 <- lm(sales~ 1 ,data = advertising[-1])

step_forward <- stats::step(step_lm_0,scope =formula(step_full),  
                            direction = "forward")
summary(step_forward)



step_backward <- stats::step(object = step_full,#scope = formula(step_lm_0) ,
                         direction = "backward")
summary(step_backward )

step_both<- stats::step(object = step_lm_0, scope = formula(step_full) ,
                         direction = "both")
summary(step_both)
```

## 模型选择和优化

![](images/clipboard-3278835564.png)

![](images/clipboard-1523534461.png)

n是观测值的数量，p是模型的参数数（等于回归系数的数量）

![](images/clipboard-4133329383.png)

![](images/clipboard-3792639976.png)

$\mathcal{L}$是模型拟合的最大似然值

```{r}
########################两模型比较
lm1 <- lm(sales~TV+radio+newspaper,data = advertising)
lm2 <- lm(sales~TV*radio*newspaper,data = advertising)

anova(lm2,lm1) #anova() 嵌套模型


##########################################            AIC 
AIC(lm2,lm1)  # 赤池信息准则  AIC值小的优先选择
#BIC


####################################相对重要性##################################
ad <- scale(advertising[-1])
ad
#R平方贡献率  #相对权重 
relweights<-function(fit,...){
  R<-cor(fit$model)
  nvar<-ncol(R)
  rxx<-R[2:nvar,2:nvar]
  rxy<-R[2:nvar,1]
  svd<-eigen(rxx)
  evec<-svd$vectors
  ev<-svd$values
  delta<-diag(sqrt(ev))
  lambda<-evec %*%delta %*% t(evec)
  lambdaasq<-lambda^2
  beta<-solve(lambda) %*% rxy
  r2<-colSums(beta^2)
  rawwgt<-lambdaasq%*%beta^2
  import<-(rawwgt/r2)*100            #计算相对权重
  import<-data.frame(Weights=import)  #数据框化
  row.names(import)<-names(fit$model[2:nvar])
  import<-import[order(import$Weights),1,drop=FALSE] #升序排序
  dotchart(import$Weights,labels=row.names(import),   #点图
           xlab = "% of R-Square",pch=19,
           main="Relative Importiance of Predictor Variables ",
           sub=paste("Total R-Square =",round(r2,digits = 3)),
  ...)
return(import)
}
relweights(lm1,col="blue")
```

## 线性可加模型

additive model

$$
Y_i=\beta_0+ \beta_1 X_i+  \beta_2 X_i^2+\epsilon_i
$$

$$
Y_i=\beta_0+ \beta_1\times \log(X_i)+\epsilon_i
$$

$$
Y_i=\beta_0+ \beta_1 (X_i\times W_i)+\epsilon_i
$$

$$
Y_i=\beta_0+ \beta_1\times \exp(X_i)+\epsilon_i
$$

$$
Y_i=\beta_0+ \beta_1\times \sin(X_i)+\epsilon_i
$$
